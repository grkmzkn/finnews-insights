{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9172774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ast\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ab9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b093f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "Train: 23603 samples\n",
      "Valid: 3062 samples\n",
      "Test: 3239 samples\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_excel(\"../data/final_train_data.xlsx\")\n",
    "df_valid = pd.read_excel(\"../data/final_valid_data.xlsx\")\n",
    "df_test = pd.read_excel(\"../data/final_test_data.xlsx\")\n",
    "\n",
    "# Function to convert string lists to actual lists\n",
    "def convert_string_to_list(string_list):\n",
    "    try:\n",
    "        return ast.literal_eval(string_list)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert string lists to actual lists for each dataset\n",
    "df_train['tokens'] = df_train['tokens'].apply(convert_string_to_list)\n",
    "df_train['tags'] = df_train['tags'].apply(convert_string_to_list)\n",
    "\n",
    "df_valid['tokens'] = df_valid['tokens'].apply(convert_string_to_list)\n",
    "df_valid['tags'] = df_valid['tags'].apply(convert_string_to_list)\n",
    "\n",
    "df_test['tokens'] = df_test['tokens'].apply(convert_string_to_list)\n",
    "df_test['tags'] = df_test['tags'].apply(convert_string_to_list)\n",
    "\n",
    "print(\"Dataset dimensions:\")\n",
    "print(f\"Train: {len(df_train)} samples\")\n",
    "print(f\"Valid: {len(df_valid)} samples\")\n",
    "print(f\"Test: {len(df_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67a1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution after filtering:\n",
      "\n",
      "Label distribution in Train dataset:\n",
      "Label 0: 499783 occurrences (84.24%)\n",
      "Label 1: 10922 occurrences (1.84%)\n",
      "Label 2: 13333 occurrences (2.25%)\n",
      "Label 3: 15429 occurrences (2.60%)\n",
      "Label 4: 11147 occurrences (1.88%)\n",
      "Label 5: 12820 occurrences (2.16%)\n",
      "Label 6: 18246 occurrences (3.08%)\n",
      "Label 7: 1763 occurrences (0.30%)\n",
      "Label 8: 2498 occurrences (0.42%)\n",
      "Label 9: 2411 occurrences (0.41%)\n",
      "Label 10: 4912 occurrences (0.83%)\n",
      "\n",
      "Label distribution in Validation dataset:\n",
      "Label 0: 63406 occurrences (83.97%)\n",
      "Label 1: 1507 occurrences (2.00%)\n",
      "Label 2: 1809 occurrences (2.40%)\n",
      "Label 3: 2020 occurrences (2.68%)\n",
      "Label 4: 1395 occurrences (1.85%)\n",
      "Label 5: 1740 occurrences (2.30%)\n",
      "Label 6: 2336 occurrences (3.09%)\n",
      "Label 7: 177 occurrences (0.23%)\n",
      "Label 8: 258 occurrences (0.34%)\n",
      "Label 9: 271 occurrences (0.36%)\n",
      "Label 10: 587 occurrences (0.78%)\n",
      "\n",
      "Label distribution in Test dataset:\n",
      "Label 0: 68294 occurrences (83.92%)\n",
      "Label 1: 1602 occurrences (1.97%)\n",
      "Label 2: 2011 occurrences (2.47%)\n",
      "Label 3: 1988 occurrences (2.44%)\n",
      "Label 4: 1412 occurrences (1.74%)\n",
      "Label 5: 1795 occurrences (2.21%)\n",
      "Label 6: 2406 occurrences (2.96%)\n",
      "Label 7: 349 occurrences (0.43%)\n",
      "Label 8: 523 occurrences (0.64%)\n",
      "Label 9: 313 occurrences (0.38%)\n",
      "Label 10: 683 occurrences (0.84%)\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution\n",
    "def check_label_distribution(df, name):\n",
    "    all_tags = []\n",
    "    for tags in df['tags']:\n",
    "        all_tags.extend(tags)\n",
    "    unique, counts = np.unique(all_tags, return_counts=True)\n",
    "    total = sum(counts)\n",
    "    print(f\"\\nLabel distribution in {name} dataset:\")\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"Label {u}: {c} occurrences ({(c/total)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nLabel distribution after filtering:\")\n",
    "check_label_distribution(df_train, \"Train\")\n",
    "check_label_distribution(df_valid, \"Validation\")\n",
    "check_label_distribution(df_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3dbed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SpaCy label mappings:\n",
      "1: DATE\n",
      "2: DATE\n",
      "3: PERSON\n",
      "4: PERSON\n",
      "5: ORG\n",
      "6: ORG\n",
      "7: PERCENT\n",
      "8: PERCENT\n",
      "9: MONEY\n",
      "10: MONEY\n",
      "\n",
      "NER labels added.\n"
     ]
    }
   ],
   "source": [
    "# Load label file and create label mappings\n",
    "with open('../data/label.json', 'r') as f:\n",
    "    label_dict = json.load(f)\n",
    "\n",
    "# Prepare label names for SpaCy (B-PERSON -> PERSON etc.)\n",
    "spacy_labels = {}\n",
    "for label, idx in label_dict.items():\n",
    "    if label != 'O':\n",
    "        entity_type = label.split('-')[1]  # B-PERSON -> PERSON\n",
    "        spacy_labels[idx] = entity_type\n",
    "\n",
    "print(\"\\nSpaCy label mappings:\")\n",
    "for idx, label in spacy_labels.items():\n",
    "    print(f\"{idx}: {label}\")\n",
    "\n",
    "# Create an empty SpaCy model\n",
    "nlp = spacy.blank(\"en\")  # Blank model for English\n",
    "\n",
    "# Add NER pipeline\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "    \n",
    "# Add labels\n",
    "for label in set(spacy_labels.values()):\n",
    "    ner.add_label(label)\n",
    "\n",
    "print(\"\\nNER labels added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93feb665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 23603\n",
      "Validation examples: 3062\n",
      "\n",
      "Example data:\n",
      "Text: Last week , Sen. Malcolm Wallop -LRB- R. , Wyo . -RRB- held hearings on a bill to strengthen an existing law designed to reduce regulatory hassles for small businesses .\n",
      "Entities: [(0, 4, 'DATE'), (5, 9, 'DATE'), (17, 24, 'PERSON'), (25, 31, 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Convert data to SpaCy format\n",
    "def convert_to_spacy_format(df):\n",
    "    training_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        text = ' '.join(row['tokens'])\n",
    "        entities = []\n",
    "        \n",
    "        # Convert tags to entity spans\n",
    "        i = 0\n",
    "        while i < len(row['tags']):\n",
    "            if row['tags'][i] != 0:  # If not O tag\n",
    "                # Find entity type\n",
    "                entity_type = spacy_labels.get(row['tags'][i])\n",
    "                \n",
    "                # Find start position\n",
    "                start_char = len(' '.join(row['tokens'][:i]))\n",
    "                if i > 0:\n",
    "                    start_char += 1  # Add space\n",
    "                    \n",
    "                # Find end position\n",
    "                end_char = start_char + len(row['tokens'][i])\n",
    "                \n",
    "                entities.append((start_char, end_char, entity_type))\n",
    "            i += 1\n",
    "        \n",
    "        training_data.append((text, {\"entities\": entities}))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Convert datasets\n",
    "train_data = convert_to_spacy_format(df_train)\n",
    "valid_data = convert_to_spacy_format(df_valid)\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(valid_data)}\")\n",
    "\n",
    "# Show first example\n",
    "print(\"\\nExample data:\")\n",
    "print(\"Text:\", train_data[0][0])\n",
    "print(\"Entities:\", train_data[0][1]['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training settings\n",
    "optimizer = nlp.begin_training()\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    \n",
    "    # Training in mini-batches\n",
    "    batches = [train_data[i:i + batch_size] for i in range(0, len(train_data), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(batches), desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                examples.append(example)\n",
    "            \n",
    "            # Train on batch\n",
    "            nlp.update(\n",
    "                examples,\n",
    "                drop=0.5,  # dropout\n",
    "                losses=losses\n",
    "            )\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Print loss at the end of epoch\n",
    "    print(f\"Epoch {epoch+1} losses:\", losses)\n",
    "    \n",
    "    # Save model checkpoint after each epoch\n",
    "    checkpoint_dir = f\"../data/model_checkpoints/checkpoint_epoch_{epoch+1}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    nlp.to_disk(str(checkpoint_dir))\n",
    "    print(f\"Checkpoint saved: {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e292d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(model, examples):\n",
    "    tp = 0  # True positives\n",
    "    fp = 0  # False positives\n",
    "    fn = 0  # False negatives\n",
    "    \n",
    "    for text, annotations in examples:\n",
    "        doc = model(text)\n",
    "        gold_entities = annotations['entities']\n",
    "        pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # True positives\n",
    "        tp += len(set(gold_entities) & set(pred_entities))\n",
    "        # False positives\n",
    "        fp += len(set(pred_entities) - set(gold_entities))\n",
    "        # False negatives\n",
    "        fn += len(set(gold_entities) - set(pred_entities))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(nlp, valid_data)\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample texts\n",
    "def test_ner(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Test texts\n",
    "test_texts = [\n",
    "    \"Apple CEO Tim Cook introduced the new iPhone model at a conference held in San Francisco.\",\n",
    "    \"Microsoft announced its quarterly earnings in January 2024.\"\n",
    "]\n",
    "\n",
    "print(\"Model test results:\\n\")\n",
    "for text in test_texts:\n",
    "    print(f\"Text: {text}\")\n",
    "    entities = test_ner(text)\n",
    "    print(\"Found entities:\")\n",
    "    for entity, label in entities:\n",
    "        print(f\"  {entity}: {label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "output_dir = \"../models/spacy_ner_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "nlp.to_disk(str(output_dir))\n",
    "print(f\"\\nModel saved to directory: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
